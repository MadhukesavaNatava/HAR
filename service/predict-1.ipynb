{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd42a968-8f46-4d12-9b73-8aac9b4ce911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "485b1280-b5bb-4a3c-8310-011163480634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TFLite version of MobileNet SSD model for human detection\n",
    "tflite_model_path = \"../model/mobilenet/1.tflite\"\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80bf8ee0-cd7f-4032-a9e7-e1f6cf4e1503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f400d1c4-e419-4138-9404-34af5652ea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3402513-cd78-4b93-800c-84d98a8ace5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input details: [{'name': 'normalized_input_image_tensor', 'index': 0, 'shape': array([1, 1, 1, 3], dtype=int32), 'shape_signature': array([ 1, -1, -1,  3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "Output details: [{'name': 'raw_outputs/box_encodings', 'index': 298, 'shape': array([ 1, 33,  4], dtype=int32), 'shape_signature': array([ 1, -1,  4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'raw_outputs/class_predictions', 'index': 302, 'shape': array([ 1, 33, 91], dtype=int32), 'shape_signature': array([ 1, -1, 91], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "Required input shape: [1 1 1 3]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 271\u001b[0m\n\u001b[0;32m    268\u001b[0m     process_video(video_path, system)\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 271\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 268\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    265\u001b[0m system \u001b[38;5;241m=\u001b[39m ActionRecognitionSystem(config, model_paths)\n\u001b[0;32m    266\u001b[0m video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/no-test.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 268\u001b[0m \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 229\u001b[0m, in \u001b[0;36mprocess_video\u001b[1;34m(video_path, system)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43msystem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;66;03m# Draw bounding boxes and predictions on the frame\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m box \u001b[38;5;129;01min\u001b[39;00m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman_boxes\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "Cell \u001b[1;32mIn[8], line 210\u001b[0m, in \u001b[0;36mActionRecognitionSystem.process_frame\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    208\u001b[0m i3d_actions \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_frames:\n\u001b[1;32m--> 210\u001b[0m     i3d_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_predictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_i3d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_people\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(human_boxes),\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman_boxes\u001b[39m\u001b[38;5;124m'\u001b[39m: human_boxes,\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcnn_actions\u001b[39m\u001b[38;5;124m'\u001b[39m: cnn_actions,\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi3d_actions\u001b[39m\u001b[38;5;124m'\u001b[39m: i3d_actions\n\u001b[0;32m    217\u001b[0m }\n",
      "Cell \u001b[1;32mIn[8], line 68\u001b[0m, in \u001b[0;36mActionPredictor.predict_i3d\u001b[1;34m(self, frames)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(frames) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_frames:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# Pad frames if necessary\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     frames\u001b[38;5;241m.\u001b[39mextend([frames[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_frames \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(frames)))\n\u001b[1;32m---> 68\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_i3d_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_frames\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(preprocessed, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[8], line 53\u001b[0m, in \u001b[0;36mActionPredictor.preprocess_i3d_frames\u001b[1;34m(self, frames)\u001b[0m\n\u001b[0;32m     51\u001b[0m     frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m     52\u001b[0m     processed_frames\u001b[38;5;241m.\u001b[39mappend(frame \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m)\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\al\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\al\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\al\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion.py:161\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m\u001b[38;5;241m.\u001b[39mtf_export(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_to_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m     97\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconvert_to_tensor_v2_with_dispatch\u001b[39m(\n\u001b[0;32m     99\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype_hint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    100\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m tensor_lib\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    101\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to a `Tensor`.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m  This function converts Python objects of various types to `Tensor`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03m    ValueError: If the `value` is a tensor not of given `dtype` in graph mode.\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_tensor_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype_hint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_hint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\al\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion.py:171\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# preferred_dtype = preferred_dtype or dtype_hint\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_hint\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\al\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[0;32m    225\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    226\u001b[0m           _add_error_prefix(\n\u001b[0;32m    227\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    231\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 234\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m    237\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\al\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_tensor_conversion.py:29\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m constant_op  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[0;32m     28\u001b[0m _ \u001b[38;5;241m=\u001b[39m as_ref\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\al\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    cnn_img_size: Tuple[int, int] = (64, 64)\n",
    "    i3d_frame_size: Tuple[int, int] = (224, 224)\n",
    "    max_frames: int = 32\n",
    "    confidence_threshold: float = 0.2\n",
    "    human_class_id: int = 1\n",
    "    ssd_input_size: Tuple[int, int] = (300, 300)\n",
    "\n",
    "@dataclass\n",
    "class ModelPaths:\n",
    "    ssd_model: str\n",
    "    cnn_model: str\n",
    "    i3d_model: str\n",
    "    label_map: str\n",
    "\n",
    "class ActionPredictor:\n",
    "    def __init__(self, config: ModelConfig, model_paths: ModelPaths):\n",
    "        self.config = config\n",
    "        self.cnn_model = joblib.load(model_paths.cnn_model)\n",
    "        self.i3d_model = self._load_i3d_model(model_paths.i3d_model)\n",
    "        self.labels = self._load_labels(model_paths.label_map)\n",
    "        self.cnn_labels = {\n",
    "            0: 'clapping', \n",
    "            1: 'dancing', \n",
    "            2: 'laughing', \n",
    "            3: 'running'\n",
    "        }\n",
    "\n",
    "    def _load_i3d_model(self, model_dir: str) -> tf.keras.Model:\n",
    "        \"\"\"Load I3D model with proper configuration.\"\"\"\n",
    "        return tf.compat.v1.saved_model.load_v2(model_dir, tags=['train'])\n",
    "\n",
    "    def _load_labels(self, label_file: str) -> List[str]:\n",
    "        \"\"\"Load action labels from file.\"\"\"\n",
    "        with open(label_file, \"r\") as f:\n",
    "            return [line.strip() for line in f.readlines()]\n",
    "\n",
    "    def preprocess_cnn_frame(self, frame: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Preprocess frame for CNN model.\"\"\"\n",
    "        frame = cv2.resize(frame, self.config.cnn_img_size)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frame = img_to_array(frame) / 255.0\n",
    "        return np.expand_dims(frame, axis=0)\n",
    "\n",
    "    def preprocess_i3d_frames(self, frames: List[np.ndarray]) -> tf.Tensor:\n",
    "        \"\"\"Preprocess frame sequence for I3D model.\"\"\"\n",
    "        processed_frames = []\n",
    "        for frame in frames:\n",
    "            frame = cv2.resize(frame, self.config.i3d_frame_size)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            processed_frames.append(frame / 255.0)\n",
    "        return tf.convert_to_tensor(processed_frames, dtype=tf.float32)\n",
    "\n",
    "    def predict_cnn(self, frame: np.ndarray) -> Tuple[str, float]:\n",
    "        \"\"\"Predict action using CNN model.\"\"\"\n",
    "        preprocessed = self.preprocess_cnn_frame(frame)\n",
    "        predictions = self.cnn_model.predict(preprocessed)\n",
    "        class_idx = np.argmax(predictions, axis=1)[0]\n",
    "        return self.cnn_labels[class_idx], predictions[0][class_idx]\n",
    "\n",
    "    def predict_i3d(self, frames: List[np.ndarray]) -> Dict[str, float]:\n",
    "        \"\"\"Predict action using I3D model.\"\"\"\n",
    "        if len(frames) < self.config.max_frames:\n",
    "            # Pad frames if necessary\n",
    "            frames.extend([frames[-1]] * (self.config.max_frames - len(frames)))\n",
    "        \n",
    "        preprocessed = self.preprocess_i3d_frames(frames[:self.config.max_frames])\n",
    "        input_tensor = tf.expand_dims(preprocessed, axis=0)\n",
    "        \n",
    "        try:\n",
    "            signature_key = list(self.i3d_model.signatures.keys())[0]\n",
    "            predictions = self.i3d_model.signatures[signature_key](input_tensor)\n",
    "            logits = predictions['default'].numpy()[0]\n",
    "            probabilities = tf.nn.softmax(logits).numpy()\n",
    "            \n",
    "            # Get top 5 predictions\n",
    "            top_indices = np.argsort(probabilities)[-5:][::-1]\n",
    "            return {\n",
    "                self.labels[idx]: float(probabilities[idx])\n",
    "                for idx in top_indices\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"I3D prediction error: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "class HumanDetector:\n",
    "    def __init__(self, model_path: str, config: ModelConfig):\n",
    "        self.config = config\n",
    "        self.interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "        self.interpreter.allocate_tensors()\n",
    "        self.input_details = self.interpreter.get_input_details()\n",
    "        self.output_details = self.interpreter.get_output_details()\n",
    "        \n",
    "        # Print model details for debugging\n",
    "        print(\"Input details:\", self.input_details)\n",
    "        print(\"Output details:\", self.output_details)\n",
    "        \n",
    "        # Get actual required input shape from model\n",
    "        self.input_shape = self.input_details[0]['shape']\n",
    "        print(f\"Required input shape: {self.input_shape}\")\n",
    "\n",
    "    def preprocess_frame(self, frame: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Preprocess frame for MobileNet SSD model:\n",
    "        - Resize to required dimensions\n",
    "        - Convert to float32\n",
    "        - Normalize to [0, 1]\n",
    "        \"\"\"\n",
    "        # Resize frame\n",
    "        input_frame = cv2.resize(frame, (self.input_shape[1], self.input_shape[2]))\n",
    "        \n",
    "        # Convert to float32 and normalize to [0, 1]\n",
    "        input_frame = input_frame.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Add batch dimension\n",
    "        input_tensor = np.expand_dims(input_frame, axis=0)\n",
    "        \n",
    "        return input_tensor\n",
    "\n",
    "    def detect(self, frame: np.ndarray) -> List[Tuple[int, int, int, int]]:\n",
    "        \"\"\"Detect humans in frame using MobileNet SSD.\"\"\"\n",
    "        if frame is None or frame.size == 0:\n",
    "            return []\n",
    "\n",
    "        original_h, original_w = frame.shape[:2]\n",
    "        \n",
    "        # Preprocess the frame\n",
    "        input_tensor = self.preprocess_frame(frame)\n",
    "\n",
    "        # Set the input tensor\n",
    "        self.interpreter.set_tensor(self.input_details[0]['index'], input_tensor)\n",
    "        self.interpreter.invoke()\n",
    "\n",
    "        # Get output tensors\n",
    "        detection_output = None\n",
    "        for output in self.output_details:\n",
    "            tensor = self.interpreter.get_tensor(output['index'])\n",
    "            if len(tensor.shape) == 3 and tensor.shape[2] == 4:  # This is likely our detection tensor\n",
    "                detection_output = tensor[0]\n",
    "            \n",
    "        if detection_output is None:\n",
    "            print(\"Could not find detection output tensor\")\n",
    "            return []\n",
    "\n",
    "        human_boxes = []\n",
    "        num_detections = len(detection_output)\n",
    "        \n",
    "        for i in range(num_detections):\n",
    "            detection = detection_output[i]\n",
    "            score = detection[2]  # Typically: [ymin, xmin, score, class_id]\n",
    "            class_id = int(detection[3])\n",
    "            \n",
    "            if score > self.config.confidence_threshold and class_id == self.config.human_class_id:\n",
    "                # Get coordinates\n",
    "                ymin, xmin = detection[0], detection[1]\n",
    "                ymax, xmax = ymin + 0.1, xmin + 0.1  # Default small box if height/width not provided\n",
    "                \n",
    "                # Convert normalized coordinates to pixel coordinates\n",
    "                startX = int(xmin * original_w)\n",
    "                startY = int(ymin * original_h)\n",
    "                endX = int(xmax * original_w)\n",
    "                endY = int(ymax * original_h)\n",
    "                \n",
    "                # Ensure coordinates are within image bounds\n",
    "                startX = max(0, startX)\n",
    "                startY = max(0, startY)\n",
    "                endX = min(original_w, endX)\n",
    "                endY = min(original_h, endY)\n",
    "                \n",
    "                # Only add valid boxes\n",
    "                if endX > startX and endY > startY:\n",
    "                    human_boxes.append((startX, startY, endX, endY))\n",
    "\n",
    "        return human_boxes\n",
    "\n",
    "class ActionRecognitionSystem:\n",
    "    def __init__(self, config: ModelConfig, model_paths: ModelPaths):\n",
    "        self.config = config\n",
    "        self.human_detector = HumanDetector(model_paths.ssd_model, config)\n",
    "        self.action_predictor = ActionPredictor(config, model_paths)\n",
    "        self.frame_buffer = []\n",
    "\n",
    "    def process_frame(self, frame: np.ndarray) -> Dict:\n",
    "        \"\"\"Process a single frame and return detections and actions.\"\"\"\n",
    "        if frame is None or frame.size == 0:\n",
    "            return {}\n",
    "\n",
    "        # Buffer frames for I3D\n",
    "        self.frame_buffer.append(frame.copy())\n",
    "        if len(self.frame_buffer) > self.config.max_frames:\n",
    "            self.frame_buffer.pop(0)\n",
    "\n",
    "        # Detect humans\n",
    "        human_boxes = self.human_detector.detect(frame)\n",
    "        \n",
    "        # Process individual humans with CNN\n",
    "        cnn_actions = []\n",
    "        for box in human_boxes:\n",
    "            startX, startY, endX, endY = box\n",
    "            if all(x >= 0 for x in [startX, startY, endX, endY]):\n",
    "                human_frame = frame[startY:endY, startX:endX]\n",
    "                if human_frame.size > 0:\n",
    "                    action, confidence = self.action_predictor.predict_cnn(human_frame)\n",
    "                    cnn_actions.append((action, confidence))\n",
    "\n",
    "        # Process scene-level actions with I3D if enough frames\n",
    "        i3d_actions = {}\n",
    "        if len(self.frame_buffer) >= self.config.max_frames:\n",
    "            i3d_actions = self.action_predictor.predict_i3d(self.frame_buffer)\n",
    "\n",
    "        return {\n",
    "            'num_people': len(human_boxes),\n",
    "            'human_boxes': human_boxes,\n",
    "            'cnn_actions': cnn_actions,\n",
    "            'i3d_actions': i3d_actions\n",
    "        }\n",
    "\n",
    "def process_video(video_path: str, system: ActionRecognitionSystem):\n",
    "    \"\"\"Process video file and display frame-by-frame analysis.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    try:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            result = system.process_frame(frame)\n",
    "            \n",
    "            # Draw bounding boxes and predictions on the frame\n",
    "            for box in result['human_boxes']:\n",
    "                startX, startY, endX, endY = box\n",
    "                cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "            \n",
    "            # Display CNN predictions\n",
    "            for i, (action, confidence) in enumerate(result['cnn_actions']):\n",
    "                cv2.putText(frame, f\"Person {i+1}: {action} ({confidence:.2f})\", \n",
    "                            (10, 30 + i * 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            \n",
    "            # Display I3D predictions\n",
    "            y_offset = 30 + len(result['cnn_actions']) * 30\n",
    "            for i, (action, confidence) in enumerate(result['i3d_actions'].items()):\n",
    "                cv2.putText(frame, f\"Scene: {action} ({confidence:.2f})\", \n",
    "                            (10, y_offset + i * 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            \n",
    "            # Display the frame\n",
    "            cv2.imshow(\"Frame\", frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "def main():\n",
    "    config = ModelConfig()\n",
    "    model_paths = ModelPaths(\n",
    "        ssd_model=\"../model/mobilenet/1.tflite\",\n",
    "        cnn_model=\"../model/cnn/HAR_CNN.joblib\",\n",
    "        i3d_model=\"../model/i3d/\",\n",
    "        label_map=\"../model/i3d/label_map.txt\"\n",
    "    )\n",
    "\n",
    "    system = ActionRecognitionSystem(config, model_paths)\n",
    "    video_path = \"../data/no-test.mp4\"\n",
    "    \n",
    "    process_video(video_path, system)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6476f63-a20e-4a54-84e4-9f0967dabd56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
